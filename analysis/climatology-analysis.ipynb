{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import pickle\n",
    "from copy import deepcopy\n",
    "\n",
    "import iris\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from scipy import ndimage as nd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm import tqdm\n",
    "\n",
    "from wildfires.analysis.plotting import partial_dependence_plot\n",
    "from wildfires.data.cube_aggregation import aggregate_cubes\n",
    "from wildfires.data.datasets import DATA_DIR, data_map_plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregate_cubes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", \".*Collapsing a non-contiguous coordinate.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def log_mapping(key):\n",
    "    if \"log\" in key:\n",
    "        return False\n",
    "    if key.lower() in {\"monthly burned area\", \"popd\"}:\n",
    "        return True\n",
    "    if \" \".join(key.lower().split(\" \")[1:]) in {\"monthly burned area\", \"popd\"}:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(os.path.join(DATA_DIR, \"monthly_climatologies.pickle\"), \"rb\") as f:\n",
    "    mean_cubes = pickle.load(f)\n",
    "print(mean_cubes)\n",
    "# so that analysis below can be replicated for other kinds of cubes\n",
    "cubes = mean_cubes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get land mask from the data\n",
    "mpl.rcParams[\"figure.figsize\"] = (8, 5)\n",
    "\n",
    "land_cube = cubes.extract_strict(iris.Constraint(name=\"pftNoLand\"))\n",
    "land_mask = np.isclose(land_cube.data.data, 1.0)\n",
    "fig = data_map_plot(np.ma.mean(land_mask, axis=0), name=\"Land Mask\")\n",
    "\n",
    "# remove this cube from the CubeList\n",
    "del cubes[cubes.index(land_cube)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a latitude mask which ignores data beyond 60 degrees, as the precipitation data has strange artifacts at those latitudes.\n",
    "lats = cubes[0].coord(\"latitude\").points\n",
    "lons = cubes[1].coord(\"longitude\").points\n",
    "lat_mask = (\n",
    "    np.zeros_like(land_mask) + (np.meshgrid(lats, lons, indexing=\"ij\")[0] > 60)[None]\n",
    ")\n",
    "\n",
    "lat_land_cubes = deepcopy(cubes)\n",
    "\n",
    "for cube in lat_land_cubes:\n",
    "    cube.data.mask[lat_mask] = True\n",
    "    cube.data.mask[land_mask] = True\n",
    "\n",
    "n_cols = 4\n",
    "n_plots = len(lat_land_cubes)\n",
    "\n",
    "mpl.rcParams[\"figure.figsize\"] = (20, 12)\n",
    "\n",
    "fig, axes = plt.subplots(\n",
    "    nrows=int(np.ceil(float(n_plots) / n_cols)), ncols=n_cols, squeeze=False\n",
    ")\n",
    "axes = axes.flatten()\n",
    "for (i, (ax, feature)) in enumerate(zip(axes, range(n_plots))):\n",
    "    ax.hist(\n",
    "        lat_land_cubes[feature].data.data[~lat_land_cubes[feature].data.mask],\n",
    "        density=True,\n",
    "        bins=70,\n",
    "    )\n",
    "    ax.set_xlabel(lat_land_cubes[feature].name())\n",
    "    ax.set_yscale(\"log\")\n",
    "\n",
    "for ax in axes[n_plots:]:\n",
    "    ax.set_axis_off()\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Raw Data\n",
    "mpl.rcParams[\"figure.figsize\"] = (10, 7)\n",
    "for cube in lat_land_cubes:\n",
    "    fig = data_map_plot(\n",
    "        cube.collapsed(\"time\", iris.analysis.MEAN), log=log_mapping(cube.name())\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that all the cubes have masks\n",
    "assert np.all([hasattr(cube.data, \"mask\") for cube in cubes])\n",
    "# Respect the masking of 'monthly burned area' and ignore all others - for all others, replace\n",
    "# masked data using nearest-neighbour interpolation.\n",
    "# Thereafter, apply the land_mask and lat_mask, so that only data over land and within the latitude limits is considered.\n",
    "# Latitude limits due to anomalous behaviour of precipitation data, as well as limitations of the lightning LIS/OTD dataset.\n",
    "\n",
    "burned_area_cube = cubes.extract_strict(iris.Constraint(name=\"monthly burned area\"))\n",
    "burned_area_mask = burned_area_cube.data.mask\n",
    "combined_mask = burned_area_mask | land_mask | lat_mask\n",
    "\n",
    "cubes_mod = deepcopy(cubes)\n",
    "assert isinstance(cubes_mod, iris.cube.CubeList)\n",
    "datasets_botched = 0\n",
    "for cube in cubes_mod:\n",
    "    # In this part, data gaps are filled, so that the maximum possible area of data (limited by where burned area data is available)\n",
    "    # is used for the analysis.\n",
    "    # Choose to fill the gaps using nearest-neighbour interpolation.\n",
    "    # To do this, define a mask which will tell the algorithm where to replace data.\n",
    "\n",
    "    # Ignore burned area in this step, as this should never be modified!\n",
    "    if cube.name() != \"monthly burned area\":\n",
    "        print(cube.name())\n",
    "        # Replace data where it is masked.\n",
    "        fill_mask = cube.data.mask\n",
    "\n",
    "        # Additional data replacing for datasets below.\n",
    "        if cube.name() == \"SIF\":\n",
    "            # Replace where it is above 20.\n",
    "            fill_mask |= cube.data.data > 20\n",
    "            # Replace where it is below 0.\n",
    "            fill_mask |= cube.data.data < 0\n",
    "            datasets_botched += 1\n",
    "        elif cube.name() == \"Combined Flash Rate Time Series\":\n",
    "            # Replace where it is below 0.\n",
    "            fill_mask |= cube.data.data < 0\n",
    "            datasets_botched += 1\n",
    "\n",
    "        orig_data = cube.data.data.copy()\n",
    "        if np.any(fill_mask):\n",
    "            print(\n",
    "                \"Filling {:} elements ({:} after final masking).\".format(\n",
    "                    np.sum(fill_mask), np.sum(fill_mask[~combined_mask])\n",
    "                )\n",
    "            )\n",
    "            filled_data = cube.data.data[\n",
    "                tuple(\n",
    "                    nd.distance_transform_edt(\n",
    "                        fill_mask, return_distances=False, return_indices=True\n",
    "                    )\n",
    "                )\n",
    "            ]\n",
    "            assert np.all(np.isclose(cube.data.data[~fill_mask], orig_data[~fill_mask]))\n",
    "\n",
    "            selected_unfilled_data = orig_data[~combined_mask]\n",
    "            selected_filled_data = filled_data[~combined_mask]\n",
    "\n",
    "            print(\n",
    "                \"Min {:0.1e}/{:0.1e}, max {:0.1e}/{:0.1e} before/after filling (for relevant regions)\".format(\n",
    "                    np.min(selected_unfilled_data),\n",
    "                    np.min(selected_filled_data),\n",
    "                    np.max(selected_unfilled_data),\n",
    "                    np.max(selected_filled_data),\n",
    "                )\n",
    "            )\n",
    "        else:\n",
    "            # Prevent overwriting with previous loop's filled data.\n",
    "            filled_data = orig_data\n",
    "\n",
    "        # Always apply global combined mask.\n",
    "        cube.data = np.ma.MaskedArray(filled_data, mask=combined_mask)\n",
    "        print(\"\")\n",
    "    else:\n",
    "        # Always apply global combined mask.\n",
    "        cube.data.mask = combined_mask\n",
    "\n",
    "# Assert that both SIF and Flash Rates were handled.\n",
    "assert datasets_botched == 2\n",
    "\n",
    "# Check that there aren't any inf's or nan's in the data.\n",
    "for cube in cubes_mod:\n",
    "    assert not np.any(np.isinf(cube.data.data[~cube.data.mask]))\n",
    "    assert not np.any(np.isnan(cube.data.data[~cube.data.mask]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl.rcParams[\"figure.figsize\"] = (10, 7)\n",
    "for cube in cubes_mod:\n",
    "    fig = data_map_plot(\n",
    "        cube.collapsed(\"time\", iris.analysis.MEAN), log=log_mapping(cube.name())\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_cols = 4\n",
    "n_plots = len(cubes_mod)\n",
    "\n",
    "mpl.rcParams[\"figure.figsize\"] = (20, 12)\n",
    "\n",
    "fig, axes = plt.subplots(\n",
    "    nrows=int(np.ceil(float(n_plots) / n_cols)), ncols=n_cols, squeeze=False\n",
    ")\n",
    "axes = axes.flatten()\n",
    "for (i, (ax, feature)) in enumerate(zip(axes, range(n_plots))):\n",
    "    ax.hist(\n",
    "        cubes_mod[feature].data.data[~cubes_mod[feature].data.mask],\n",
    "        density=True,\n",
    "        bins=70,\n",
    "    )\n",
    "    ax.set_xlabel(cubes_mod[feature].name())\n",
    "    ax.set_yscale(\"log\")\n",
    "\n",
    "for ax in axes[n_plots:]:\n",
    "    ax.set_axis_off()\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "burned_area_cube = cubes_mod.extract_strict(iris.Constraint(name=\"monthly burned area\"))\n",
    "endog_data = pd.Series(burned_area_cube.data.data[~burned_area_cube.data.mask])\n",
    "names = []\n",
    "data = []\n",
    "for cube in cubes_mod:\n",
    "    if cube.name() != \"monthly burned area\":\n",
    "        names.append(cube.name())\n",
    "        data.append(cube.data.data[~cube.data.mask].reshape(-1, 1))\n",
    "exog_data = pd.DataFrame(np.hstack(data), columns=names)\n",
    "exog_data[\"temperature range\"] = (\n",
    "    exog_data[\"maximum temperature\"] - exog_data[\"minimum temperature\"]\n",
    ")\n",
    "del exog_data[\"minimum temperature\"]\n",
    "\n",
    "print(names)\n",
    "\n",
    "# Carry out log transformation for select variables.\n",
    "log_var_names = [\"temperature range\", \"dry_days\", \"dry_day_period\"]\n",
    "\n",
    "for name in log_var_names:\n",
    "    mod_data = exog_data[name] + 0.01\n",
    "    assert np.all(mod_data >= (0.01 - 1e-8)), \"{:}\".format(name)\n",
    "    exog_data[\"log \" + name] = np.log(mod_data)\n",
    "    del exog_data[name]\n",
    "\n",
    "# Carry out square root transformation\n",
    "sqrt_var_names = [\"Combined Flash Rate Time Series\", \"popd\"]\n",
    "for name in sqrt_var_names:\n",
    "    assert np.all(exog_data[name] >= 0), \"{:}\".format(name)\n",
    "    exog_data[\"sqrt \" + name] = np.sqrt(exog_data[name])\n",
    "    del exog_data[name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sm.GLM(endog_data, exog_data, family=sm.families.Binomial())\n",
    "model_results = model.fit()\n",
    "print(model_results.summary())\n",
    "\n",
    "mpl.rcParams[\"figure.figsize\"] = (12, 9)\n",
    "\n",
    "plt.figure()\n",
    "plt.hexbin(endog_data, model_results.fittedvalues, bins=\"log\")\n",
    "plt.xlabel(\"real data\")\n",
    "plt.ylabel(\"prediction\")\n",
    "plt.colorbar()\n",
    "\n",
    "global_mask = burned_area_cube.data.mask\n",
    "\n",
    "ba_predicted = np.zeros_like(global_mask, dtype=np.float64)\n",
    "ba_predicted[~global_mask] = model_results.fittedvalues\n",
    "ba_predicted = np.ma.MaskedArray(ba_predicted, mask=global_mask)\n",
    "fig = data_map_plot(\n",
    "    np.ma.mean(ba_predicted, axis=0), name=\"Predicted Mean Burned Area\", log=True\n",
    ")\n",
    "\n",
    "ba_data = np.zeros_like(global_mask, dtype=np.float64)\n",
    "ba_data[~global_mask] = endog_data.values\n",
    "ba_data = np.ma.MaskedArray(ba_data, mask=global_mask)\n",
    "fig = data_map_plot(\n",
    "    np.ma.mean(ba_data, axis=0), name=\"Mean observed burned area (GFEDv4)\", log=True\n",
    ")\n",
    "\n",
    "_ = plt.matshow(exog_data.corr())\n",
    "_ = plt.xticks(range(len(exog_data.columns)), exog_data.columns, rotation=\"vertical\")\n",
    "_ = plt.yticks(range(len(exog_data.columns)), exog_data.columns)\n",
    "_ = plt.colorbar()\n",
    "\n",
    "print(\"R2:\", r2_score(y_true=endog_data, y_pred=model_results.fittedvalues))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove redundant variables: Soil Water Index, LAI, precip\n",
    "exog_data2 = deepcopy(exog_data)\n",
    "del exog_data2[\"Soil Water Index with T=1\"]\n",
    "del exog_data2[\"Leaf Area Index\"]\n",
    "del exog_data2[\"precip\"]\n",
    "\n",
    "model = sm.GLM(endog_data, exog_data2, family=sm.families.Binomial())\n",
    "model_results = model.fit()\n",
    "print(model_results.summary())\n",
    "\n",
    "mpl.rcParams[\"figure.figsize\"] = (12, 9)\n",
    "\n",
    "plt.figure()\n",
    "plt.hexbin(endog_data, model_results.fittedvalues, bins=\"log\")\n",
    "plt.xlabel(\"real data\")\n",
    "plt.ylabel(\"prediction\")\n",
    "plt.colorbar()\n",
    "\n",
    "global_mask = burned_area_cube.data.mask\n",
    "\n",
    "ba_predicted = np.zeros_like(global_mask, dtype=np.float64)\n",
    "ba_predicted[~global_mask] = model_results.fittedvalues\n",
    "ba_predicted = np.ma.MaskedArray(ba_predicted, mask=global_mask)\n",
    "fig = data_map_plot(\n",
    "    np.ma.mean(ba_predicted, axis=0), name=\"Predicted Mean Burned Area\", log=True\n",
    ")\n",
    "\n",
    "ba_data = np.zeros_like(global_mask, dtype=np.float64)\n",
    "ba_data[~global_mask] = endog_data.values\n",
    "ba_data = np.ma.MaskedArray(ba_data, mask=global_mask)\n",
    "fig = data_map_plot(\n",
    "    np.ma.mean(ba_data, axis=0), name=\"Mean observed burned area (GFEDv4)\", log=True\n",
    ")\n",
    "\n",
    "columns = exog_data2.columns\n",
    "_ = plt.matshow(exog_data2.corr())\n",
    "_ = plt.xticks(range(len(columns)), columns, rotation=\"vertical\")\n",
    "_ = plt.yticks(range(len(columns)), columns)\n",
    "_ = plt.colorbar()\n",
    "\n",
    "print(\"R2:\", r2_score(y_true=endog_data, y_pred=model_results.fittedvalues))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regr = RandomForestRegressor(n_estimators=50, random_state=1, n_jobs=4)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    exog_data2, endog_data, random_state=1, shuffle=True, test_size=0.3\n",
    ")\n",
    "regr.fit(X_train, y_train)\n",
    "print(\"R2 train:\", regr.score(X_train, y_train))\n",
    "print(\"R2 test:\", regr.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl.rcParams[\"figure.figsize\"] = (20, 12)\n",
    "from importlib import reload\n",
    "\n",
    "import wildfires.analysis.plotting\n",
    "\n",
    "reload(wildfires.analysis.plotting)\n",
    "fig, axes = wildfires.analysis.plotting.partial_dependence_plot(\n",
    "    regr,\n",
    "    X_test,\n",
    "    X_test.columns,\n",
    "    n_cols=4,\n",
    "    grid_resolution=50,\n",
    "    coverage=0.02,\n",
    "    random_state=1,\n",
    "    predicted_name=\"burned area\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl.rcParams[\"figure.figsize\"] = (12, 9)\n",
    "\n",
    "plt.figure()\n",
    "plt.hexbin(\n",
    "    y_test,\n",
    "    regr.predict(X_test),\n",
    "    bins=\"log\",\n",
    "    # xscale='log', yscale='log'\n",
    ")\n",
    "plt.xlabel(\"real data\")\n",
    "plt.ylabel(\"prediction\")\n",
    "plt.colorbar()\n",
    "\n",
    "mpl.rcParams[\"figure.figsize\"] = (12, 10)\n",
    "\n",
    "global_mask = burned_area_cube.data.mask\n",
    "\n",
    "ba_predicted = np.zeros_like(global_mask, dtype=np.float64)\n",
    "ba_predicted[~global_mask] = regr.predict(exog_data2)\n",
    "ba_predicted = np.ma.MaskedArray(ba_predicted, mask=global_mask)\n",
    "fig = data_map_plot(\n",
    "    np.ma.mean(ba_predicted, axis=0), name=\"Predicted Mean Burned Area\", log=True\n",
    ")\n",
    "\n",
    "ba_data = np.zeros_like(global_mask, dtype=np.float64)\n",
    "ba_data[~global_mask] = endog_data\n",
    "ba_data = np.ma.MaskedArray(ba_data, mask=global_mask)\n",
    "fig = data_map_plot(\n",
    "    np.ma.mean(ba_data, axis=0), name=\"Mean observed burned area (GFEDv4)\", log=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mpl.rcParams[\"figure.figsize\"] = (20, 12)\n",
    "\n",
    "n_cols = 4\n",
    "\n",
    "fig, axes = plt.subplots(\n",
    "    nrows=int(np.ceil(float(len(exog_data2.columns)) / n_cols)),\n",
    "    ncols=n_cols,\n",
    "    squeeze=False,\n",
    ")\n",
    "axes = axes.flatten()\n",
    "for (i, (ax, feature)) in enumerate(zip(axes, exog_data2.columns)):\n",
    "    ax.hist(exog_data2[feature], density=True, bins=70)\n",
    "    ax.set_xlabel(feature)\n",
    "    ax.set_yscale(\"log\")\n",
    "\n",
    "for ax in axes[len(exog_data2.columns) :]:\n",
    "    ax.set_axis_off()\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "print(\"{:<59s} | {:<10s}\".format(\"Name\", \"VIF\"))\n",
    "for i, name in enumerate(exog_data.columns):\n",
    "    print(\n",
    "        \"{:<59s} | {:>10.1f}\".format(\n",
    "            name, variance_inflation_factor(exog_data.values, i)\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exog_data.values.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"{:<59s} | {:<10s}\".format(\"Name\", \"VIF\"))\n",
    "for i, name in enumerate(exog_data.columns):\n",
    "    X_fit = exog_data.values[:, i].reshape(-1, 1)\n",
    "    X_k = exog_data.values[:, [j for j in range(len(exog_data.columns)) if j != i]]\n",
    "    X_k = np.hstack((np.ones(X_k.shape[0]).reshape(-1, 1), X_k))\n",
    "\n",
    "    x, res, rank, s = np.linalg.lstsq(X_k, X_fit, rcond=None)\n",
    "    predicted = X_k.dot(x)\n",
    "\n",
    "    vif = 1.0 / (1 - r2_score(y_true=X_fit, y_pred=predicted))\n",
    "\n",
    "    # r2 = r2_score(y_true=X_fit, y_pred=predicted)\n",
    "    # alt_r2 = OLS(X_fit, X_k).fit().rsquared\n",
    "    # print('{:+>5.1e} {:+>5.1e}'.format(r2, alt_r2))\n",
    "    # The r2 scores above do match, but the VIF values do not. This is likely due to\n",
    "    # the fact that the statsmodels implementation uses the OLS linear fit procedure,\n",
    "    # which does not add a constant by default (as was done explicitly above using\n",
    "    # np.ones()). This is the only difference I could find, as the methods are otherwise\n",
    "    # identical.\n",
    "    print(\"{:<59s} | {:>10.1f}\".format(name, vif))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"After variable removal\")\n",
    "print(\"{:<59s} | {:<10s}\".format(\"Name\", \"VIF\"))\n",
    "for i, name in enumerate(exog_data2.columns):\n",
    "    X_fit = exog_data2.values[:, i].reshape(-1, 1)\n",
    "    X_k = exog_data2.values[:, [j for j in range(len(exog_data2.columns)) if j != i]]\n",
    "    X_k = np.hstack((np.ones(X_k.shape[0]).reshape(-1, 1), X_k))\n",
    "\n",
    "    x, res, rank, s = np.linalg.lstsq(X_k, X_fit, rcond=None)\n",
    "    predicted = X_k.dot(x)\n",
    "\n",
    "    vif = 1.0 / (1 - r2_score(y_true=X_fit, y_pred=predicted))\n",
    "\n",
    "    # r2 = r2_score(y_true=X_fit, y_pred=predicted)\n",
    "    # alt_r2 = OLS(X_fit, X_k).fit().rsquared\n",
    "    # print('{:+>5.1e} {:+>5.1e}'.format(r2, alt_r2))\n",
    "    # The r2 scores above do match, but the VIF values do not. This is likely due to\n",
    "    # the fact that the statsmodels implementation uses the OLS linear fit procedure,\n",
    "    # which does not add a constant by default (as was done explicitly above using\n",
    "    # np.ones()). This is the only difference I could find, as the methods are otherwise\n",
    "    # identical.\n",
    "    print(\"{:<59s} | {:>10.1f}\".format(name, vif))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from statsmodels.regression.linear_model import OLS\n",
    "\n",
    "i = 0\n",
    "\n",
    "X_fit = exog_data.values[:, i].reshape(-1, 1)\n",
    "X_k = exog_data.values[:, [j for j in range(len(exog_data.columns)) if j != i]]\n",
    "X_k = np.hstack((np.ones(X_k.shape[0]).reshape(-1, 1), X_k))\n",
    "\n",
    "x, res, rank, s = np.linalg.lstsq(X_k, X_fit, rcond=None)\n",
    "predicted1 = X_k.dot(x)\n",
    "\n",
    "alt_r2 = OLS(X_fit, X_k).fit().rsquared\n",
    "results = model.fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(x.shape[0]):\n",
    "    print(\"{:0.1e} {:0.1e}\".format(x.flatten()[i], results.params[i]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
