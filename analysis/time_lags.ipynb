{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "import os\n",
    "\n",
    "import matplotlib as mpl\n",
    "import numpy as np\n",
    "from joblib import Memory\n",
    "\n",
    "from wildfires.analysis.analysis import *\n",
    "from wildfires.analysis.plotting import *\n",
    "from wildfires.data.cube_aggregation import *\n",
    "from wildfires.data.datasets import *\n",
    "from wildfires.logging_config import enable_logging\n",
    "\n",
    "FigureSaver.directory = os.path.expanduser(os.path.join(\"~\", \"tmp\", \"to_send2\"))\n",
    "os.makedirs(FigureSaver.directory, exist_ok=True)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "enable_logging(\"jupyter\")\n",
    "warnings.filterwarnings(\"ignore\", \".*Collapsing a non-contiguous coordinate.*\")\n",
    "warnings.filterwarnings(\"ignore\", \".*DEFAULT_SPHERICAL_EARTH_RADIUS*\")\n",
    "warnings.filterwarnings(\"ignore\", \".*guessing contiguous bounds*\")\n",
    "\n",
    "normal_coast_linewidth = 0.5\n",
    "mpl.rc(\"figure\", figsize=(14, 6))\n",
    "mpl.rc(\"font\", size=9.0)\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "\n",
    "location = os.path.join(DATA_DIR, \"joblib_cachedir\")\n",
    "memory = Memory(location, verbose=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the Data Structures used for Fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@memory.cache\n",
    "def get_data(shift_months=None):\n",
    "    target_variable = \"GFED4 BA\"\n",
    "\n",
    "    # Creation of new variables.\n",
    "    transformations = {\n",
    "        \"Temp Range\": lambda exog_data: (exog_data[\"Max Temp\"] - exog_data[\"Min Temp\"])\n",
    "    }\n",
    "    # Variables to be deleted after the aforementioned transformations.\n",
    "    deletions = (\"Min Temp\",)\n",
    "\n",
    "    # Carry out transformations, replacing old variables in the process.\n",
    "    # log_var_names = [\"Temp Range\", \"Dry Day Period\"]\n",
    "    # sqrt_var_names = [\n",
    "    #     # \"Lightning Climatology\",\n",
    "    #     \"popd\"\n",
    "    # ]\n",
    "\n",
    "    # Dataset selection.\n",
    "\n",
    "    # TODO: Make this selection process more elegant.\n",
    "\n",
    "    selection_datasets = [\n",
    "        AvitabileThurnerAGB(),\n",
    "        CHELSA(),\n",
    "        Copernicus_SWI(),\n",
    "        ERA5_CAPEPrecip(),\n",
    "        ERA5_DryDayPeriod(),\n",
    "        ESA_CCI_Landcover_PFT(),\n",
    "        GFEDv4(),\n",
    "        GlobFluo_SIF(),\n",
    "        HYDE(),\n",
    "        MOD15A2H_LAI_fPAR(),\n",
    "        VODCA(),\n",
    "    ]\n",
    "    if shift_months is not None:\n",
    "        datasets_to_shift = (ERA5_DryDayPeriod, MOD15A2H_LAI_fPAR, VODCA)\n",
    "        for shift in shift_months:\n",
    "            for shift_dataset in datasets_to_shift:\n",
    "                selection_datasets.append(\n",
    "                    shift_dataset.get_temporally_shifted_dataset(months=-shift)\n",
    "                )\n",
    "\n",
    "    selection_variables = [\n",
    "        \"AGB Tree\",\n",
    "        \"Max Temp\",\n",
    "        \"Min Temp\",\n",
    "        \"SWI(1)\",\n",
    "        \"CAPE x Precip\",\n",
    "        \"Dry Day Period\",\n",
    "        \"ShrubAll\",\n",
    "        \"TreeAll\",\n",
    "        \"pftCrop\",\n",
    "        \"pftHerb\",\n",
    "        \"GFED4 BA\",\n",
    "        \"SIF\",\n",
    "        \"popd\",\n",
    "        \"FAPAR\",\n",
    "        \"LAI\",\n",
    "        \"VOD Ku-band\",\n",
    "    ]\n",
    "    if shift_months is not None:\n",
    "        for shift in shift_months:\n",
    "            selection_variables.extend(\n",
    "                [\n",
    "                    f\"LAI {-shift} Month\",\n",
    "                    f\"FAPAR {-shift} Month\",\n",
    "                    f\"Dry Day Period {-shift} Month\",\n",
    "                    f\"VOD Ku-band {-shift} Month\",\n",
    "                ]\n",
    "            )\n",
    "\n",
    "    selection = Datasets(selection_datasets).select_variables(selection_variables)\n",
    "    (\n",
    "        endog_data,\n",
    "        exog_data,\n",
    "        master_mask,\n",
    "        filled_datasets,\n",
    "        masked_datasets,\n",
    "        land_mask,\n",
    "    ) = data_processing(\n",
    "        selection,\n",
    "        which=\"climatology\",\n",
    "        transformations=transformations,\n",
    "        deletions=deletions,\n",
    "        # log_var_names=log_var_names,\n",
    "        # sqrt_var_names=sqrt_var_names,\n",
    "        use_lat_mask=False,\n",
    "        use_fire_mask=False,\n",
    "        target_variable=target_variable,\n",
    "    )\n",
    "    return (\n",
    "        endog_data,\n",
    "        exog_data,\n",
    "        master_mask,\n",
    "        filled_datasets,\n",
    "        masked_datasets,\n",
    "        land_mask,\n",
    "    )\n",
    "\n",
    "\n",
    "(\n",
    "    endog_data,\n",
    "    exog_data,\n",
    "    master_mask,\n",
    "    filled_datasets,\n",
    "    masked_datasets,\n",
    "    land_mask,\n",
    ") = get_data()\n",
    "\n",
    "(\n",
    "    s_endog_data,\n",
    "    s_exog_data,\n",
    "    s_master_mask,\n",
    "    s_filled_datasets,\n",
    "    s_masked_datasets,\n",
    "    s_land_mask,\n",
    ") = get_data(shift_months=[1, 3, 6, 12, 24])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Optimisation Using CX1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from wildfires.analysis.cx1_fitting import CX1Fit\n",
    "\n",
    "# Define the training and test data.\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    exog_data, endog_data, random_state=1, shuffle=True, test_size=0.3\n",
    ")\n",
    "\n",
    "# Define the parameter space.\n",
    "parameters_RF = {\n",
    "    \"n_estimators\": [10, 50, 100],\n",
    "    \"max_depth\": [None, 10, 20],\n",
    "    \"min_samples_split\": [2, 5, 10],\n",
    "    \"min_samples_leaf\": [3, 10, 20],\n",
    "    \"max_features\": [\"auto\"],\n",
    "    \"bootstrap\": [False, True],\n",
    "    \"random_state\": [1],\n",
    "}\n",
    "\n",
    "fitting = CX1Fit(X_train, y_train, data_name=\"full_no_shift\", param_grid=parameters_RF)\n",
    "fitting.run_job()\n",
    "output = fitting.get_best_model(timeout=60 * 60)\n",
    "if output:\n",
    "    regr = output[\"model\"]\n",
    "\n",
    "    print(estimator)\n",
    "    y_pred = regr.predict(X_test)\n",
    "\n",
    "    # Carry out predictions on the training dataset to diagnose overfitting.\n",
    "    y_pred_train = regr.predict(X_train)\n",
    "\n",
    "    results = {}\n",
    "    results[\"R2_train\"] = regr.score(X_train, y_train)\n",
    "    results[\"R2_test\"] = regr.score(X_test, y_test)\n",
    "\n",
    "    model_name = \"RF\"\n",
    "    print(f\"{model_name} R2 train: {results['R2_train']}\")\n",
    "    print(f\"{model_name} R2 test: {results['R2_test']}\")\n",
    "\n",
    "    importances = regr.feature_importances_\n",
    "    std = np.std([tree.feature_importances_ for tree in regr.estimators_], axis=0)\n",
    "\n",
    "    importances_df = pd.DataFrame(\n",
    "        {\n",
    "            \"Name\": exog_data.columns.values,\n",
    "            \"Importance\": importances,\n",
    "            \"Importance STD\": std,\n",
    "            \"Ratio\": np.array(std) / np.array(importances),\n",
    "        }\n",
    "    )\n",
    "    print(\n",
    "        \"\\n\"\n",
    "        + str(\n",
    "            importances_df.sort_values(\"Importance\", ascending=False).to_string(\n",
    "                index=False, float_format=\"{:0.3f}\".format, line_width=200\n",
    "            )\n",
    "        )\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:wildfires]",
   "language": "python",
   "name": "conda-env-wildfires-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
