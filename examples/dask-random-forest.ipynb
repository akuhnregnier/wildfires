{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RF Hyperparameter Optimisation using Dask - Comparing Different Approaches"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "from time import time\n",
    "\n",
    "import numpy as np\n",
    "from dask.distributed import Client, as_completed\n",
    "from joblib import parallel_backend\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "\n",
    "class Time:\n",
    "    def __init__(self, name=\"\"):\n",
    "        self.name = name\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.start = time()\n",
    "\n",
    "    def __exit__(self, type, value, traceback):\n",
    "        print(\"Time taken for {}: {}\".format(self.name, time() - self.start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Optimisation Using Dask on CX1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Used for `score()` on the current node (i.e. not using Dask).\n",
    "local_n_jobs = 5\n",
    "\n",
    "client = Client(\n",
    "    n_workers=5, threads_per_worker=5\n",
    ")  # Create a LocalCluster for demonstration purposes.\n",
    "client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Common Parameters and Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the common training and test data.\n",
    "np.random.seed(1)\n",
    "X = np.random.random((int(2e3), 40))\n",
    "y = X[:, 0] + X[:, 1] + np.random.random((X.shape[0],))\n",
    "\n",
    "# Define the number of splits.\n",
    "n_splits = 5\n",
    "kf = KFold(n_splits=n_splits)\n",
    "\n",
    "# Define the parameter space.\n",
    "parameters_RF = {\n",
    "    \"n_estimators\": [50],\n",
    "    \"max_depth\": [6, 9, 12],\n",
    "    \"min_samples_split\": [2],\n",
    "    \"min_samples_leaf\": [1, 5, 10],\n",
    "}\n",
    "\n",
    "default_param_dict = {\n",
    "    \"random_state\": 1,\n",
    "    \"bootstrap\": True,\n",
    "    \"max_features\": \"auto\",\n",
    "}\n",
    "\n",
    "rf_params_list = [\n",
    "    dict(zip(parameters_RF, param_values))\n",
    "    for param_values in product(*parameters_RF.values())\n",
    "]\n",
    "\n",
    "rf_params = default_param_dict.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define our own RF implementation that submits individual trees as Dask tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wildfires.dask_cx1 import DaskRandomForestRegressor, fit_dask_rf_grid_search_cv\n",
    "\n",
    "with Time(\"Custom Dask Gridsearch\"):\n",
    "    results = fit_dask_rf_grid_search_cv(\n",
    "        DaskRandomForestRegressor(**default_param_dict),\n",
    "        X,\n",
    "        y,\n",
    "        n_splits,\n",
    "        parameters_RF,\n",
    "        client,\n",
    "        verbose=True,\n",
    "        return_train_score=True,\n",
    "        refit=False,\n",
    "        local_n_jobs=local_n_jobs,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform individual fits in series\n",
    "\n",
    "Wait for each RF fit to complete (using the Dask backend) and score (using local threading backend, since `predict()` (used by `score()` requires 'sharedmem'!) before starting the next one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_and_score(X, y, train_index, test_index, rf_params):\n",
    "    rf = RandomForestRegressor(**rf_params)\n",
    "    with parallel_backend(\"dask\"):\n",
    "\n",
    "        rf.fit(X[train_index], y[train_index])\n",
    "\n",
    "    with parallel_backend(\"threading\", n_jobs=local_n_jobs):\n",
    "        test_score = rf.score(X[test_index], y[test_index])\n",
    "        train_score = rf.score(X[train_index], y[train_index])\n",
    "\n",
    "    return test_score, train_score\n",
    "\n",
    "\n",
    "test_scores_list = []\n",
    "train_scores_list = []\n",
    "\n",
    "with Time(\"In Series\"):\n",
    "    for rf_grid_params in tqdm(rf_params_list, desc=\"Params\"):\n",
    "        rf_params.update(rf_grid_params)\n",
    "        test_scores = []\n",
    "        train_scores = []\n",
    "        for i, (train_index, test_index) in enumerate(list(kf.split(X))):\n",
    "            test_score, train_score = fit_and_score(\n",
    "                X, y, train_index, test_index, rf_params\n",
    "            )\n",
    "            test_scores.append(test_score)\n",
    "            train_scores.append(train_score)\n",
    "\n",
    "        test_scores_list.append(test_scores)\n",
    "        train_scores_list.append(train_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dask-ML GridSearchCV\n",
    "\n",
    "This works, but only allocates one thread per **forest fit**, _not per tree_, making for very slow training when `n_fits < n_workers`.\n",
    "\n",
    "Only use this when `n_fits >> n_workers`, where `n_fits = n_parameters * n_splits`, or when individual model `fit()` calls are only single threaded (**unlike** `RandomForestRegressor.fit()`, which releases the GIL)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dask_ml.model_selection import GridSearchCV\n",
    "\n",
    "gs = GridSearchCV(\n",
    "    RandomForestRegressor(random_state=1, bootstrap=True, max_features=\"auto\"),\n",
    "    parameters_RF,\n",
    "    cv=n_splits,\n",
    "    return_train_score=True,\n",
    "    refit=False,\n",
    ")\n",
    "with Time(\"Dask-ML GridSearchCV\"):\n",
    "    gs = gs.fit(X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Native sklearn GridSearchCV fails with CancelledError\n",
    "\n",
    "It is apparent (prior to failing) that this does spread out the training of individual trees, which should have lead to expected speedups when `n_fits < n_workers` (or about the same magnitude).\n",
    "\n",
    "The CancelledError occurrence has already been reported:\n",
    " - https://github.com/scikit-learn/scikit-learn/issues/12315\n",
    " - https://github.com/scikit-learn/scikit-learn/issues/15383\n",
    " - https://github.com/joblib/joblib/issues/959\n",
    " - https://github.com/joblib/joblib/issues/1021"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "gs = GridSearchCV(\n",
    "    RandomForestRegressor(random_state=1, bootstrap=True, max_features=\"auto\"),\n",
    "    parameters_RF,\n",
    "    cv=n_splits,\n",
    "    return_train_score=True,\n",
    "    refit=False,\n",
    ")\n",
    "with Time(\"Scikit-learn GridSearchCV with Dask\"):\n",
    "    with parallel_backend(\"dask\"):\n",
    "        gs = gs.fit(X, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
